{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Required Libraries to run code\n",
    "import glob\n",
    "import cv2 as cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Read Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading of Vehicle Images Done\n",
      "Reading of Non Vehicle Images Done\n",
      "No of Vehicle Images Loaded -0\n",
      "No of Non-Vehicle Images Loaded -0\n"
     ]
    }
   ],
   "source": [
    "#reading image paths with glob\n",
    "vehicle_image_arr = glob.glob('./vehicles/vehicles/*/*.png')\n",
    "\n",
    "# read images and append to list\n",
    "vehicle_images_original=[]\n",
    "for imagePath in vehicle_image_arr:\n",
    "    readImage=cv2.imread(imagePath)\n",
    "    rgbImage = cv2.cvtColor(readImage, cv2.COLOR_BGR2RGB)\n",
    "    vehicle_images_original.append(rgbImage)\n",
    "\n",
    "print('Reading of Vehicle Images Done')\n",
    "\n",
    "non_vehicle_image_arr = glob.glob('./non-vehicles/non-vehicles/*/*.png')\n",
    "\n",
    "\n",
    "non_vehicle_images_original=[]\n",
    "for imagePath in non_vehicle_image_arr:\n",
    "    readImage=cv2.imread(imagePath)\n",
    "    rgbImage = cv2.cvtColor(readImage, cv2.COLOR_BGR2RGB)\n",
    "    non_vehicle_images_original.append(rgbImage)\n",
    "\n",
    "print(\"Reading of Non Vehicle Images Done\")\n",
    "\n",
    "print(\"No of Vehicle Images Loaded -\"+ str(len(vehicle_image_arr)))\n",
    "print(\"No of Non-Vehicle Images Loaded -\"+ str(len(non_vehicle_images_original)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Vehicle and Non Vehicle Images\n",
    "\n",
    "f, axes = plt.subplots(4,2, figsize=(10,10))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for index in range(4):\n",
    "    vehicle=random.randint(0, len(vehicle_images_original)-1)\n",
    "    non_vehicle=random.randint(0, len(non_vehicle_images_original)-1)\n",
    "    axes[index,0].imshow(vehicle_images_original[vehicle])\n",
    "    axes[index,0].set_title(\"Vehicle\")\n",
    "    axes[index,1].imshow(non_vehicle_images_original[non_vehicle])\n",
    "    axes[index,1].set_title(\"Non Vehicle\")\n",
    "print(\"Shape of Vehicle Image\" +  str(vehicle_images_original[vehicle].shape))\n",
    "print(\"Shape of Non Vehicle Image\" +  str(non_vehicle_images_original[non_vehicle].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2- Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 - Color Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract Color Space\n",
    "\n",
    "#creating a Histogram\n",
    "def ExtractColorHistogram(image, nbins=32, bins_range=(0,255), resize=None):\n",
    "    if(resize !=None):\n",
    "        image= cv2.resize(image, resize)\n",
    "    zero_channel= np.histogram(image[:,:,0], bins=nbins, range=bins_range)\n",
    "    first_channel= np.histogram(image[:,:,1], bins=nbins, range=bins_range)\n",
    "    second_channel= np.histogram(image[:,:,2], bins=nbins, range=bins_range)\n",
    "    return zero_channel,first_channel, second_channel\n",
    "\n",
    "#Find Center of the bin edges\n",
    "def FindBinCenter(histogram_channel):\n",
    "    bin_edges = histogram_channel[1]\n",
    "    bin_centers = (bin_edges[1:]  + bin_edges[0:len(bin_edges)-1])/2\n",
    "    return bin_centers\n",
    "\n",
    "#Extracting Color Features from bin lengths\n",
    "def ExtractColorFeatures(zero_channel, first_channel, second_channel):\n",
    "    return np.concatenate((zero_channel[0], first_channel[0], second_channel[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Color Features for Vehicles\n",
    "\n",
    "f, axes= plt.subplots(4,5, figsize=(20,10))\n",
    "f.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for index in range(4):\n",
    "    \n",
    "    vehicle=random.randint(0, len(vehicle_images_original)-1)\n",
    "    non_vehicle=random.randint(0, len(non_vehicle_images_original)-1)\n",
    "    \n",
    "    coloredImage= cv2.cvtColor(vehicle_images_original[vehicle],cv2.COLOR_RGB2YUV)\n",
    "    r,g,b = ExtractColorHistogram(coloredImage,128)\n",
    "   \n",
    "    center= FindBinCenter(r)\n",
    "    axes[index,0].imshow(vehicle_images_original[vehicle])\n",
    "    axes[index,0].set_title(\"Vehicle\")\n",
    "    axes[index,1].set_xlim(0,256)\n",
    "    axes[index,1].bar(center,r[0])\n",
    "    axes[index,1].set_title(\"Y\")\n",
    "    axes[index,2].set_xlim(0,256)\n",
    "    axes[index,2].bar(center,g[0])\n",
    "    axes[index,2].set_title(\"U\")\n",
    "    axes[index,3].set_xlim(0,256)\n",
    "    axes[index,3].bar(center,b[0])\n",
    "    axes[index,3].set_title(\"V\")\n",
    "    axes[index,4].imshow(coloredImage)\n",
    "    axes[index,4].set_title(\"YUV colorspace\")\n",
    "    \n",
    "features = ExtractColorFeatures(r,g,b)\n",
    "print(\"No of features are \"+ str(len(features)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Color Features for Non Vehicles\n",
    "\n",
    "f, axes= plt.subplots(4,5, figsize=(20,10))\n",
    "f.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for index in range(4):\n",
    "    non_vehicle=random.randint(0, len(non_vehicle_images_original)-1)\n",
    "    coloredImage= cv2.cvtColor(non_vehicle_images_original[non_vehicle],cv2.COLOR_RGB2YUV)\n",
    "    r,g,b = ExtractColorHistogram(coloredImage)\n",
    "    \n",
    "    center= FindBinCenter(r)\n",
    "    axes[index,0].imshow(non_vehicle_images_original[non_vehicle])\n",
    "    axes[index,0].set_title(\"Non Vehicle\")\n",
    "    axes[index,1].set_xlim(0,256)\n",
    "    axes[index,1].bar(center,r[0])\n",
    "    axes[index,1].set_title(\"Y\")\n",
    "    axes[index,2].set_xlim(0,256)\n",
    "    axes[index,2].bar(center,g[0])\n",
    "    axes[index,2].set_title(\"U\")\n",
    "    axes[index,3].set_xlim(0,256)\n",
    "    axes[index,3].bar(center,b[0])\n",
    "    axes[index,3].set_title(\"V\")\n",
    "    axes[index,4].imshow(coloredImage)\n",
    "    axes[index,4].set_title(\"YUV colorspace\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2 - Spatial Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resizing Image to extract features, so as to reduce the feature vector size\n",
    "def SpatialBinningFeatures(image,size):\n",
    "    image= cv2.resize(image,size)\n",
    "    return image.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the spatial binning\n",
    "\n",
    "featureList=SpatialBinningFeatures(vehicle_images_original[1],(16,16))\n",
    "print(\"No of features before spatial binning\",len(vehicle_images_original[1].ravel()))\n",
    "print(\"No of features after spatial binning\",len(featureList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3 - HOG ( Histogram of Oriented Gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General method to extact the HOG of the image\n",
    "\n",
    "def GetFeaturesFromHog(image,orient,cellsPerBlock,pixelsPerCell, visualise= False, feature_vector_flag=True):\n",
    "    if(visualise==True):\n",
    "        hog_features, hog_image = hog(image, orientations=orient,\n",
    "                          pixels_per_cell=(pixelsPerCell, pixelsPerCell), \n",
    "                          cells_per_block=(cellsPerBlock, cellsPerBlock), \n",
    "                          visualise=True, feature_vector=feature_vector_flag)\n",
    "        return hog_features, hog_image\n",
    "    else:\n",
    "        hog_features = hog(image, orientations=orient,\n",
    "                          pixels_per_cell=(pixelsPerCell, pixelsPerCell), \n",
    "                          cells_per_block=(cellsPerBlock, cellsPerBlock), \n",
    "                          visualise=False, feature_vector=feature_vector_flag)\n",
    "        return hog_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing HOG on test images\n",
    "\n",
    "image=vehicle_images_original[1]\n",
    "image= cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "image_channel_0=image[:,:,0]\n",
    "image_channel_1=image[:,:,0]\n",
    "image_channel_2=image[:,:,0]\n",
    "\n",
    "feature_0,hog_img_0=GetFeaturesFromHog(image_channel_0,9,2,16,visualise=True,feature_vector_flag=True)\n",
    "feature_1,hog_img_1=GetFeaturesFromHog(image_channel_1,9,2,16,visualise=True,feature_vector_flag=True)\n",
    "feature_2,hog_img_2=GetFeaturesFromHog(image_channel_2,9,2,16,visualise=True,feature_vector_flag=True)\n",
    "\n",
    "f, axes= plt.subplots(1,4,figsize=(20,10))\n",
    "axes[0].imshow(vehicle_images_original[1])\n",
    "axes[1].imshow(hog_img_0)\n",
    "axes[2].imshow(hog_img_1)\n",
    "axes[3].imshow(hog_img_2)\n",
    "\n",
    "\n",
    "print(\"Feature Vector Length Returned is \",len(feature_0))\n",
    "print(\"No of features that can be extracted from image \",len(hog_img_0.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3- Generate Features Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Image Color Space. Note the colorspace parameter is like cv2.COLOR_RGB2YUV\n",
    "def ConvertImageColorspace(image, colorspace):\n",
    "    return cv2.cvtColor(image, colorspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to extract the features based on the choices as available in step 2\n",
    "\n",
    "def ExtractFeatures(images,orientation,cellsPerBlock,pixelsPerCell, convertColorspace=False):\n",
    "    featureList=[]\n",
    "    imageList=[]\n",
    "    for image in images:\n",
    "        if(convertColorspace==True):\n",
    "            image= cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "        local_features_1=GetFeaturesFromHog(image[:,:,0],orientation,cellsPerBlock,pixelsPerCell, False, True)\n",
    "        local_features_2=GetFeaturesFromHog(image[:,:,1],orientation,cellsPerBlock,pixelsPerCell, False, True)\n",
    "        local_features_3=GetFeaturesFromHog(image[:,:,2],orientation,cellsPerBlock,pixelsPerCell, False, True)\n",
    "        x=np.hstack((local_features_1,local_features_2,local_features_3))\n",
    "        featureList.append(x)\n",
    "    return featureList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the Features of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "orientations=9\n",
    "cellsPerBlock=2\n",
    "pixelsPerBlock=16\n",
    "convertColorSpace=True\n",
    "vehicleFeatures= ExtractFeatures(vehicle_images_original,orientations,cellsPerBlock,pixelsPerBlock, convertColorSpace)\n",
    "nonVehicleFeatures= ExtractFeatures(non_vehicle_images_original,orientations,cellsPerBlock,pixelsPerBlock, convertColorSpace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "featuresList= np.vstack([vehicleFeatures, nonVehicleFeatures])\n",
    "print(\"Shape of features list is \", featuresList.shape)\n",
    "labelList= np.concatenate([np.ones(len(vehicleFeatures)), np.zeros(len(nonVehicleFeatures))])\n",
    "print(\"Shape of label list is \", labelList.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4- Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1 - Splitting Data into Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split of data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,  X_test,Y_train, Y_test = train_test_split(featuresList, labelList, test_size=0.2, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2 - Normalization and Scaling of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization and scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler= StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled= scaler.transform(X_train)\n",
    "X_test_scaled= scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5- Define and Train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train a Linear SVM classifer\n",
    "from sklearn.svm import LinearSVC\n",
    "classifier1= LinearSVC()\n",
    "classifier1.fit(X_train,Y_train)\n",
    "print(\"Accuracy of SVC is  \", classifier1.score(X_test,Y_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to draw sliding Windows\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    \n",
    "    for bbox in bboxes:\n",
    "        r=random.randint(0,255)\n",
    "        g=random.randint(0,255)\n",
    "        b=random.randint(0,255)\n",
    "        color=(r, g, b)\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the windows on which we are going to run the classifier\n",
    "\n",
    "def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.9, 0.9)):\n",
    "   \n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0]=0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1]=img.shape[1]\n",
    "    if y_start_stop[0] ==  None:\n",
    "        y_start_stop[0]= 0\n",
    "    if y_start_stop[1] ==  None:\n",
    "        y_start_stop[1]=img.shape[0]\n",
    "    \n",
    "    \n",
    "    window_list = []\n",
    "    image_width_x= x_start_stop[1] - x_start_stop[0]\n",
    "    image_width_y= y_start_stop[1] - y_start_stop[0]\n",
    "     \n",
    "    windows_x = np.int( 1 + (image_width_x - xy_window[0])/(xy_window[0] * xy_overlap[0]))\n",
    "    windows_y = np.int( 1 + (image_width_y - xy_window[1])/(xy_window[1] * xy_overlap[1]))\n",
    "    \n",
    "    modified_window_size= xy_window\n",
    "    for i in range(0,windows_y):\n",
    "        y_start = y_start_stop[0] + np.int( i * modified_window_size[1] * xy_overlap[1])\n",
    "        for j in range(0,windows_x):\n",
    "            x_start = x_start_stop[0] + np.int( j * modified_window_size[0] * xy_overlap[0])\n",
    "            \n",
    "            x1 = np.int( x_start +  modified_window_size[0])\n",
    "            y1= np.int( y_start + modified_window_size[1])\n",
    "            window_list.append(((x_start,y_start),(x1,y1)))\n",
    "    return window_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns the refined Windows\n",
    "# From Refined Windows we mean that the windows where the classifier predicts the output to be a car\n",
    "\n",
    "def DrawCars(image,windows, converColorspace=False):\n",
    "    refinedWindows=[]\n",
    "    for window in windows:\n",
    "        \n",
    "        start= window[0]\n",
    "        end= window[1]\n",
    "        clippedImage=image[start[1]:end[1], start[0]:end[0]]\n",
    "        \n",
    "        if(clippedImage.shape[1] == clippedImage.shape[0] and clippedImage.shape[1]!=0):\n",
    "            \n",
    "            clippedImage=cv2.resize(clippedImage, (64,64))\n",
    "            \n",
    "            f1=ExtractFeatures([clippedImage], 9 , 2 , 16,converColorspace)\n",
    "        \n",
    "            predictedOutput=classifier1.predict([f1[0]])\n",
    "            if(predictedOutput==1):\n",
    "                refinedWindows.append(window)\n",
    "        \n",
    "    return refinedWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying out SubSampling using HOG but not able to go through as feature size is not the same.\n",
    "\n",
    "def DrawCarsOptimised(image, image1, image2,windows, converColorspace=False):\n",
    "    refinedWindows=[]\n",
    "    for window in windows:\n",
    "        \n",
    "        start= window[0]\n",
    "        end= window[1]\n",
    "        clippedImage=image[start[1]:end[1], start[0]:end[0]]\n",
    "        clippedImage1=image1[start[1]:end[1], start[0]:end[0]]\n",
    "        clippedImage2=image2[start[1]:end[1], start[0]:end[0]]\n",
    "        \n",
    "        if(clippedImage.shape[1] == clippedImage.shape[0] and clippedImage.shape[1]!=0):\n",
    "            \n",
    "            clippedImage=cv2.resize(clippedImage, (64,64)).ravel()\n",
    "            clippedImage1=cv2.resize(clippedImage1, (64,64)).ravel()\n",
    "            clippedImage2=cv2.resize(clippedImage2, (64,64)).ravel()\n",
    "            \n",
    "            #f1=ExtractFeatures([clippedImage], 9 , 2 , 16,converColorspace)\n",
    "            f1= np.hstack((clippedImage,clippedImage1,clippedImage2))\n",
    "            f1=scaler.transform(f1.reshape(1,-1))   \n",
    "            print(f1.shape)\n",
    "            predictedOutput=classifier1.predict([f1[0]])\n",
    "            if(predictedOutput==1):\n",
    "                refinedWindows.append(window)\n",
    "        \n",
    "    return refinedWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#testing our functions of slide_window and draw window. Defining here dummy windows\n",
    "\n",
    "image = mpimg.imread('test3.jpg')\n",
    "\n",
    "windows1 = slide_window(image, x_start_stop=[0, 1280], y_start_stop=[400,464], \n",
    "                    xy_window=(64,64), xy_overlap=(0.15, 0.15))\n",
    "windows4 = slide_window(image, x_start_stop=[0, 1280], y_start_stop=[400,480], \n",
    "                    xy_window=(80,80), xy_overlap=(0.2, 0.2))\n",
    "windows2 = slide_window(image, x_start_stop=[0, 1280], y_start_stop=[400,612], \n",
    "                    xy_window=(96,96), xy_overlap=(0.3, 0.3))\n",
    "windows3 = slide_window(image, x_start_stop=[0, 1280], y_start_stop=[400,660], \n",
    "                    xy_window=(128,128), xy_overlap=(0.5, 0.5))\n",
    "\n",
    "\n",
    "windows = windows1 + windows2 +  windows3 + windows4\n",
    "print(\"Total No of windows are \",len(windows))\n",
    "refinedWindows=DrawCars(image,windows, True)\n",
    "\n",
    "\n",
    "\n",
    "f,axes= plt.subplots(2,1, figsize=(30,15))\n",
    "\n",
    "window_img = draw_boxes(image, windows) \n",
    "\n",
    "axes[0].imshow(window_img)\n",
    "axes[0].set_title(\"Window Coverage\")\n",
    "\n",
    "window_img = draw_boxes(image, refinedWindows) \n",
    "\n",
    "axes[1].set_title(\"Test Image with Refined Sliding Windows\")\n",
    "axes[1].imshow(window_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 - Applying Heatmap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to increase the pixel by one inside each box\n",
    "\n",
    "def add_heat(heatmap, bbox_list):\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "\n",
    "    # Return updated heatmap\n",
    "    return heatmap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying a threshold value to the image to filter out low pixel cells\n",
    "\n",
    "def apply_threshold(heatmap, threshold):\n",
    "    # Zero out pixels below the threshold\n",
    "    heatmap[heatmap <= threshold] = 0\n",
    "    # Return thresholded map\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find pixels with each car number and draw the final bounding boxes\n",
    "\n",
    "from scipy.ndimage.measurements import label\n",
    "def draw_labeled_bboxes(img, labels):\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        # Draw the box on the image\n",
    "        cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6)\n",
    "    # Return the image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#testing our heat function\n",
    "\n",
    "heat = np.zeros_like(image[:,:,0]).astype(np.float)\n",
    "\n",
    "heat = add_heat(heat,refinedWindows)\n",
    "    \n",
    "# Apply threshold to help remove false positives\n",
    "heat = apply_threshold(heat,3)\n",
    "\n",
    "# Visualize the heatmap when displaying    \n",
    "heatmap = np.clip(heat, 0, 255)\n",
    "\n",
    "heat_image=heatmap\n",
    "\n",
    "# Find final boxes from heatmap using label function\n",
    "labels = label(heatmap)\n",
    "print(\" Number of Cars found - \",labels[1])\n",
    "draw_img = draw_labeled_bboxes(np.copy(image), labels)\n",
    "\n",
    "f,axes= plt.subplots(2,1, figsize=(30,15))\n",
    "axes[0].imshow(heat_image,cmap='gray')\n",
    "axes[0].set_title(\"Heat Map Image\")\n",
    "axes[1].imshow(draw_img)\n",
    "axes[1].set_title(\"Final Image after applying Heat Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8 - Averaging Rectangles over Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a class to store the refined frames found from the last 15 frames\n",
    "\n",
    "class KeepTrack():\n",
    "    def __init__(self):\n",
    "        self.refinedWindows = [] \n",
    "        \n",
    "    def AddWindows(self, refinedWindow):\n",
    "        self.refinedWindows.append(refinedWindow)\n",
    "        frameHistory=15\n",
    "        if len(self.refinedWindows) > frameHistory:\n",
    "            self.refinedWindows = self.refinedWindows[len(self.refinedWindows)-frameHistory:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9 - Defining Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the Parameters required for the pipeline to run\n",
    "\n",
    "orientation=9 # No of orientations of HOG\n",
    "cellsPerBlock=2 # No of cells per block\n",
    "pixelsPerCell=16 # No of pixels per cell\n",
    "xy_window=(64, 64) # window Size\n",
    "xy_overlap=(0.15, 0.15) # Window Overlap. Please note this is different as provided by Udacity. Overlap of 0.15 means my windows are 85% overlapping with each other\n",
    "x_start_stop=[0, image.shape[1]] # X Coordinates to start and stop search\n",
    "y_start_stop=[400, 660] # Y Coordinates to start and stop search\n",
    "\n",
    "# Window 1- Size - 64x64 , Overlap-85%\n",
    "windows_normal = slide_window(image, x_start_stop, [400,464], \n",
    "                    xy_window, xy_overlap)\n",
    "\n",
    "# Window 2- Size - 80x80 , Overlap-80%\n",
    "xy_window_1_25= (80,80)\n",
    "xy_window_1_25_overlap=(0.2, 0.2)    \n",
    "windows_1_25 = slide_window(image, x_start_stop, [400,480], \n",
    "                    xy_window_1_25, xy_window_1_25_overlap)\n",
    "\n",
    "# Window 3- Size - 96x96 , Overlap-70%\n",
    "xy_window_1_5= (96,96)\n",
    "xy_window_1_5_overlap=(0.3, 0.3)    \n",
    "windows_1_5 = slide_window(image, x_start_stop, [400,612], \n",
    "                    xy_window_1_5, xy_window_1_5_overlap)\n",
    "\n",
    "# Window 4- Size - 128x128 , Overlap-50%\n",
    "xy_window_twice_overlap=(0.5, 0.5)    \n",
    "xy_window_twice = (128,128)\n",
    "windows_twice = slide_window(image, x_start_stop, [400,660], \n",
    "                    xy_window_twice, xy_window_twice_overlap)\n",
    "\n",
    "# Total Windows - 470\n",
    "windows= windows_normal +  windows_1_5  + windows_twice +windows_1_25\n",
    "print(\"No of Windows are \",len(windows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a pipeline for Video Frame Processing\n",
    "# Note here the track of last 15 frames is kept\n",
    "\n",
    "def Pipeline(image):\n",
    "#     features,hog_image=GetFeaturesFromHog(image[:,:,0],orientation,cellsPerBlock,pixelsPerCell, visualise= True, feature_vector_flag=False)\n",
    "#     features1,hog_image1=GetFeaturesFromHog(image[:,:,1],orientation,cellsPerBlock,pixelsPerCell, visualise= True, feature_vector_flag=False)\n",
    "#     features2,hog_image2=GetFeaturesFromHog(image[:,:,2],orientation,cellsPerBlock,pixelsPerCell, visualise= True, feature_vector_flag=False)\n",
    "#     refinedWindows=DrawCarsOptimised(hog_image,hog_image1,hog_image2,windows, True)\n",
    "    \n",
    "#     image=find_cars(image, 400, 528, 1,  orientation, pixelsPerCell, cellsPerBlock)\n",
    "#     image=find_cars(image, 400, 560, 1.25,  orientation, pixelsPerCell, cellsPerBlock)\n",
    "#     image=find_cars(image, 400, 588, 1.5,  orientation, pixelsPerCell, cellsPerBlock)\n",
    "#     image=find_cars(image, 400, 660, 2,  orientation, pixelsPerCell, cellsPerBlock)\n",
    "    rand= random.randint(0,1)\n",
    "    if(rand<0.4):\n",
    "        refinedWindows=keepTrack.refinedWindows[:-1]\n",
    "    else:\n",
    "        refinedWindows=DrawCars(image,windows, True)\n",
    "        if len(refinedWindows) > 0:\n",
    "            keepTrack.AddWindows(refinedWindows)\n",
    "    \n",
    "    #refinedWindows=DrawCars(image,windows, True)\n",
    "#     if len(refinedWindows) > 0:\n",
    "#         keepTrack.AddWindows(refinedWindows)\n",
    "            \n",
    "    heat = np.zeros_like(image[:,:,0]).astype(np.float)\n",
    "    \n",
    "    for refinedWindow in keepTrack.refinedWindows:\n",
    "        heat = add_heat(heat, refinedWindow)\n",
    "    \n",
    "    \n",
    "    \n",
    "    heatmap = apply_threshold(heat, 25 + len(keepTrack.refinedWindows)//2)\n",
    "    \n",
    "    labels = label(heatmap)\n",
    "    draw_img = draw_labeled_bboxes(np.copy(image), labels)\n",
    "    return draw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a different pipeline to process the images as we do not want to keep track of previous frames here\n",
    "\n",
    "def PipelineImage(image):\n",
    "\n",
    "    refinedWindows=DrawCars(image,windows, True)\n",
    "    heat = np.zeros_like(image[:,:,0]).astype(np.float)\n",
    "    heat = add_heat(heat,refinedWindows)\n",
    "   \n",
    "    heatmap = np.clip(heat, 0, 255)\n",
    "    heatmap = apply_threshold(heat, 4)\n",
    "    labels = label(heatmap)\n",
    "    draw_img = draw_labeled_bboxes(np.copy(image), labels)\n",
    "    return draw_img,heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10- Testing Pipeline on Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "test_images= glob.glob(\"./test_images/*.jpg\")\n",
    "f, axes= plt.subplots(8,3, figsize=(20,40))\n",
    "\n",
    "for index,image in enumerate(test_images):\n",
    "    image = cv2.imread(image)\n",
    "    \n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    finalPic,heatmap = PipelineImage(image)\n",
    "    axes[index,0].imshow(image)\n",
    "    axes[index,0].set_title(\"Original Image\")\n",
    "    axes[index,1].imshow(heatmap,cmap='gray')\n",
    "    axes[index,1].set_title(\"Heatmap Image\")\n",
    "    axes[index,2].imshow(finalPic)\n",
    "    axes[index,2].set_title(\"Final Image\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11- Testing Pipeline on Test Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keepTrack = KeepTrack()\n",
    "import moviepy\n",
    "from moviepy.editor import VideoFileClip\n",
    "video_output1 = 'full_video_threshold_20_with_frame_skipping_my.mp4'\n",
    "video_input1 = VideoFileClip('project_video.mp4')\n",
    "processed_video = video_input1.fl_image(Pipeline)\n",
    "%time processed_video.write_videofile(video_output1, audio=False)\n",
    "video_input1.reader.close()\n",
    "video_input1.audio.reader.close_proc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
